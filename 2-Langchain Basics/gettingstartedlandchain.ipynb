{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97604962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3baca133",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "## Langsmith Tracking and Tracing\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a23d1939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10f57b140> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f2e1700> root_client=<openai.OpenAI object at 0x10f6cd970> root_async_client=<openai.AsyncOpenAI object at 0x10f57b290> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "233ac884",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c0057ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48fb648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n<think>\\nOkay, the user is asking for the capital of France. Let me think. I know that France is a country in Western Europe. The capital cities I remember... Paris comes to mind. Wait, is there any other possibility? Maybe Lyon? No, Lyon is a major city but not the capital. Let me verify. From what I've studied before, Paris has been the capital for a long time. It's known for the Eiffel Tower, Louvre Museum, and other landmarks. I don't think there's been any recent change. Some countries have capitals that are not the most populous cities, like Brazil's capital is BrasÃ­lia, not Rio or SÃ£o Paulo. But in France's case, Paris is both the largest city and the capital. I can't recall any historical changes to France's capital either. So I'm pretty confident the answer is Paris. Let me make sure there's no trick in the question. The user might be testing basic knowledge, so straightforward answer should be fine. Yeah, I'm pretty sure it's Paris.\\n</think>\\n\\nThe capital of France is **Paris**. It is not only the political and administrative center of the country but also a world-renowned city for its culture, history, art, and landmarks such as the Eiffel Tower and the Louvre Museum.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 267, 'prompt_tokens': 17, 'total_tokens': 284, 'completion_time': 0.655308907, 'prompt_time': 0.00310802, 'queue_time': 0.066636482, 'total_time': 0.658416927}, 'model_name': 'qwen-qwq-32B', 'system_fingerprint': 'fp_1e88ca32eb', 'finish_reason': 'stop', 'logprobs': None}, id='run--053fee49-9e4d-4d11-a22d-82e371f4fbd5-0', usage_metadata={'input_tokens': 17, 'output_tokens': 267, 'total_tokens': 284})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32B\")\n",
    "model.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5a016a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Engineer. Provide me an answer to the question.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79c7cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me an answer to the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f01df2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10f8da510>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10f8dbec0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef31efa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me an answer to the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10f8da510>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10f8dbec0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30b9d6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. ðŸ‡«ðŸ‡·  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\": \"What is the capital of France?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3af02cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI expert, I can tell you about Langsmith! \n",
      "\n",
      "Langsmith is an open-source framework developed by Replicate, designed to simplify the development and deployment of large language models (LLMs). Think of it as a toolbox specifically built for working with LLMs like your friendly neighborhood me!\n",
      "\n",
      "Here are some key features and benefits of Langsmith:\n",
      "\n",
      "**1. Streamlined Development:**\n",
      "\n",
      "* **Easy Model Access:** Langsmith makes it straightforward to connect with various LLMs, including those hosted on Replicate, Hugging Face, and your own local models.\n",
      "* **Modular Building Blocks:** It provides reusable components and templates, allowing you to quickly assemble and experiment with different LLM applications.\n",
      "* **Interactive Debugging:**  Langsmith offers interactive tools for debugging and fine-tuning your LLM workflows, making the development process more efficient.\n",
      "\n",
      "**2. Enhanced Deployment:**\n",
      "\n",
      "* **Serverless Deployment:**  You can easily deploy your LLM applications serverlessly, leveraging cloud platforms like AWS Lambda or Replicate's own serverless infrastructure.\n",
      "* **Scalability:** Langsmith is designed to scale with your needs, allowing you to handle increasing traffic and computational demands.\n",
      "* **Monitoring and Logging:** It provides tools for monitoring the performance and health of your deployed models, ensuring smooth operation.\n",
      "\n",
      "**3. Open-Source and Community-Driven:**\n",
      "\n",
      "* **Transparency:** The open-source nature of Langsmith allows for community scrutiny and collaboration, leading to continuous improvement.\n",
      "* **Extensibility:** Developers can contribute new components, integrations, and features, expanding the capabilities of the framework.\n",
      "* **Support and Resources:** The Langsmith community offers support forums, documentation, and tutorials to help users get started and overcome challenges.\n",
      "\n",
      "**In essence, Langsmith aims to bridge the gap between powerful LLMs and practical application, making it easier for developers to build, deploy, and manage LLM-powered solutions.**\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about Langsmith or LLMs in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response=chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ac136bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4de9e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\": output_parser.get_format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2ebf35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': <bound method JsonOutputParser.get_format_instructions of JsonOutputParser()>}, template='Answer the user query \\n {format_instruction}\\n {query}\\n')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2eaccb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source framework for building and deploying AI applications, specifically focusing on large language models (LLMs).', 'key_features': ['Modular and extensible design', 'Support for multiple LLM backends (e.g., Llama, GPT-Neo)', 'Tools for fine-tuning and customizing LLMs', 'Deployment options for cloud, on-premise, and edge environments', 'Integration with other AI tools and services'], 'website': 'https://github.com/langsmith-ai/langsmith', 'status': 'Active development'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt | model | output_parser\n",
    "response=chain.invoke({\"query\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d08a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework that simplifies the development of applications powered by large language models (LLMs). It provides tools and abstractions for connecting LLMs with other data sources, managing their memory, and building complex conversational flows.</answer></response>\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 39, 'total_tokens': 98, 'completion_time': 0.107272727, 'prompt_time': 0.002394937, 'queue_time': 0.090622341, 'total_time': 0.109667664}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--adb0ebb2-4f05-4787-a29e-7c15ede6049e-0' usage_metadata={'input_tokens': 39, 'output_tokens': 59, 'total_tokens': 98}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1ed5ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle stand up by itself?\",\n",
       " 'punchline': 'Because it was two tired!'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07ea5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'iPhone 15 Pro Max', 'details': 'The latest flagship smartphone from Apple with advanced features and technology.', 'price_usd': 1299.99}\n"
     ]
    }
   ],
   "source": [
    "### Assignment --- Chat prompt template\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 1. Instantiate your LLM\n",
    "model = ChatOpenAI(temperature=0.0)  # deterministic output for structured tasks\n",
    "\n",
    "# 2. Define the Pydantic model for your product schema\n",
    "class Product(BaseModel):\n",
    "    name: str = Field(..., description=\"The product's official name\")\n",
    "    details: str = Field(..., description=\"A brief description of the product\")\n",
    "    price_usd: float = Field(..., description=\"The price of the product in US dollars\")\n",
    "\n",
    "# 3. Create a JsonOutputParser around that model\n",
    "parser = JsonOutputParser(pydantic_object=Product)\n",
    "\n",
    "# 4. Build a PromptTemplate that injects the parser's format instructions\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a helpful assistant that, given a product, returns valid JSON \"\n",
    "        \"conforming to the following schema:\\n\\n\"\n",
    "        \"{format_instructions}\\n\\n\"\n",
    "        \"Product: {product}\\n\"\n",
    "    ),\n",
    "    input_variables=[\"product\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# 5. Compose your chain: prompt â†’ model â†’ parser\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 6. Invoke with a product query\n",
    "result: Product = chain.invoke({\"product\": \"iPhone 15 Pro Max\"})\n",
    "print(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
