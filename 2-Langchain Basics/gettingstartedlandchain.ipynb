{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97604962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3baca133",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "## Langsmith Tracking and Tracing\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23d1939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x1097c03e0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x109cb86e0> root_client=<openai.OpenAI object at 0x109646de0> root_async_client=<openai.AsyncOpenAI object at 0x1097c04a0> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233ac884",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0057ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48fb648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n<think>\\nOkay, the user is asking for the capital of France. Let me think. I know that France is a country in Western Europe. I remember from school that the capital cities are usually where the government is based. Paris comes to mind. Wait, isn't Paris the capital? Let me confirm. Yeah, I've heard about the Eiffel Tower and the Louvre, which are in Paris. But I should make sure there hasn't been any recent changes. No, capitals don't change that often. Also, checking in my mind, other major cities in France like Marseille or Lyon, but they're not capitals. So yes, the answer should be Paris. I don't think there's any trick here. The user probably wants a straightforward answer. Just answer Paris.\\n</think>\\n\\nThe capital of France is **Paris**. Located in the north-central part of the country, it is not only the political and cultural heart of France but also a global center for art, fashion, and cuisine. Iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral are found here.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 17, 'total_tokens': 244, 'completion_time': 0.52561069, 'prompt_time': 0.003170572, 'queue_time': 0.014641113, 'total_time': 0.528781262}, 'model_name': 'qwen-qwq-32B', 'system_fingerprint': 'fp_a91d9c2cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run--d17b830a-d0c3-4278-a194-1939d022d242-0', usage_metadata={'input_tokens': 17, 'output_tokens': 227, 'total_tokens': 244})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32B\")\n",
    "model.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a016a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Engineer. Provide me an answer to the question.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79c7cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me an answer to the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01df2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10a2c7140>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10a2c6900>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef31efa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me an answer to the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10a2c7140>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10a2c6900>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b9d6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. ðŸ˜Š  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\": \"What is the capital of France?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3af02cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're asking about **Langsmith**, which is an exciting project in the world of AI! \n",
      "\n",
      "Langsmith is an open-source platform developed by the **Hugging Face** team. Think of it as a powerful toolkit specifically designed for fine-tuning and deploying large language models (LLMs). \n",
      "\n",
      "Here's a breakdown of what makes Langsmith special:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Simplified Fine-Tuning:** Langsmith streamlines the process of customizing existing LLMs for specific tasks. It provides a user-friendly interface and intuitive workflows, making fine-tuning accessible even to those without deep technical expertise.\n",
      "* **Efficient Training:** It leverages techniques like gradient accumulation and mixed precision training to optimize the fine-tuning process, reducing training time and resource consumption.\n",
      "* **Collaborative Development:** As an open-source project, Langsmith encourages community involvement and contributions.  Developers can share their fine-tuned models, datasets, and training scripts, fostering collaboration and knowledge sharing.\n",
      "* **Model Deployment:** Langsmith goes beyond fine-tuning by offering tools to easily deploy your customized LLMs as APIs or web applications. This allows you to put your trained models into action and build real-world applications.\n",
      "* **Integration with Hugging Face Ecosystem:** Langsmith seamlessly integrates with the Hugging Face Hub, a central repository for AI models and datasets. This means you can easily access pre-trained LLMs, experiment with different architectures, and share your work with the broader community.\n",
      "\n",
      "**Who Can Benefit:**\n",
      "\n",
      "Langsmith is a valuable resource for a wide range of users:\n",
      "\n",
      "* **Researchers:**  Accelerate your research by quickly fine-tuning LLMs for specific tasks and exploring new model architectures.\n",
      "* **Developers:**  Build custom AI applications powered by LLMs without needing extensive machine learning expertise.\n",
      "* **Educators:**  Introduce students to the world of LLMs and AI development through hands-on experimentation and collaborative projects.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "The Hugging Face team provides comprehensive documentation and tutorials to help you get started with Langsmith. You can find more information and resources on the official Langsmith website and the Hugging Face Hub.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or any other AI topics!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response=chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac136bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de9e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\": output_parser.get_format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2ebf35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': <bound method JsonOutputParser.get_format_instructions of JsonOutputParser()>}, template='Answer the user query \\n {format_instruction}\\n {query}\\n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eaccb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is a powerful open-source platform designed for developing and deploying large language models (LLMs). It provides a comprehensive suite of tools and resources to streamline the entire LLM lifecycle, from training and fine-tuning to deployment and monitoring.', 'key_features': ['Modular and extensible architecture', 'Support for various LLM frameworks (e.g., Transformers, ðŸ¤—)', 'Efficient training and inference pipelines', 'Built-in tools for model evaluation and debugging', 'Simplified deployment options for cloud and on-premise environments', 'Active community and open-source contributions'], 'website': 'https://github.com/langs-org/langs'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt | model | output_parser\n",
    "response=chain.invoke({\"query\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d08a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework for developing applications powered by language models. It provides tools and components for tasks like text generation, question answering, summarization, and more.</answer></response>\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 39, 'total_tokens': 87, 'completion_time': 0.087272727, 'prompt_time': 0.002365536, 'queue_time': 0.090673362, 'total_time': 0.089638263}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--70620c68-eb3a-493d-9728-5c1db8a07e3d-0' usage_metadata={'input_tokens': 39, 'output_tokens': 48, 'total_tokens': 87}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1ed5ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle stand up by itself?\",\n",
       " 'punchline': 'Because it was two tired.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07ea5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'iPhone 15 Pro Max', 'details': 'The latest flagship smartphone from Apple with advanced features and technology.', 'price_usd': 1299.99}\n"
     ]
    }
   ],
   "source": [
    "### Assignment --- Chat prompt template\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 1. Instantiate your LLM\n",
    "model = ChatOpenAI(temperature=0.0)  # deterministic output for structured tasks\n",
    "\n",
    "# 2. Define the Pydantic model for your product schema\n",
    "class Product(BaseModel):\n",
    "    name: str = Field(..., description=\"The product's official name\")\n",
    "    details: str = Field(..., description=\"A brief description of the product\")\n",
    "    price_usd: float = Field(..., description=\"The price of the product in US dollars\")\n",
    "\n",
    "# 3. Create a JsonOutputParser around that model\n",
    "parser = JsonOutputParser(pydantic_object=Product)\n",
    "\n",
    "# 4. Build a PromptTemplate that injects the parser's format instructions\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a helpful assistant that, given a product, returns valid JSON \"\n",
    "        \"conforming to the following schema:\\n\\n\"\n",
    "        \"{format_instructions}\\n\\n\"\n",
    "        \"Product: {product}\\n\"\n",
    "    ),\n",
    "    input_variables=[\"product\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# 5. Compose your chain: prompt â†’ model â†’ parser\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 6. Invoke with a product query\n",
    "result: Product = chain.invoke({\"product\": \"iPhone 15 Pro Max\"})\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4395231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
